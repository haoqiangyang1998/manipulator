import numpy as np
import torch

a = np.array(list(range(10)))
print(a)
b = np.absolute(a[range(0, 10, 2)] - a[range(1, 10, 2)]).sum(axis=-1)
print(b)

c = torch.tensor(list(range(10)))
print(c)
d = torch.absolute(c[range(0,10,2)]-c[range(1, 10, 2)]).sum(dim=-1)
print(d)
超冗余机械臂的范畴包括超冗余离散型刚性臂，连续型刚性臂和柔性机械臂，是对结构自由度大于其工作空间自由度的一类机械臂的统称，相对于传统的临界冗余和低冗余度的机械臂来说，超冗余机械臂具有更高的自由度，这给它带来了更强的灵活性和更好的弯曲性能，使其能适应更复杂的现实任务。同时，超冗余机械臂除了主要的超冗余特性之外，往往伴随有非线性、时变等特点，这使得传统的基于精确建模的控制理论往往难以发挥作用。

为了有效解决超冗余机械臂的控制难的问题，本文提出基于深度强化学习的超冗余机械臂控制方法，其中，深度神经网络可以有效对高度非线性化和非结构化建模，而强化学习的基于数据驱动的控制方式，可以有效的跳过高冗余度带来的传统模型控制器复杂难以设计的问题。基于上述前提，本文在搭建强化学习训练环境时，跳过了柔性和弹性材料的建模，将问题简化为超冗余刚性臂的控制问题，这样可以方便建模仿真，也能抓住主要矛盾，着眼于解决超冗余状态下巨大状态空间对带来的挑战。本文主要的研究内容为无障碍条件下超冗余机械臂末端对于任意目标点的接近任务学习和存在环境障碍条件下超冗余机械臂避障接近确定目标点的任务学习，具体的研究工作如下：

首先，搭建了12节24自由度的超冗余机械臂的刚性仿真模型，为了提高模型仿真速度以适应强化学习对于巨大交互样本数据的需求，对仿真环境进行来三次主要迭代，包括基于Vrep平台的remote
API的模型控制方法，基于CoppeliaSim平台和pyrep库的脚本控制方式和基于Mujoco平台Unity引擎的仿真模型，三次仿真模型的迭代使得仿真速度有了近千倍的提高，适应了强化学习算法的需求。

其次，针对无障碍下末端接近任务的学习，本文首先分析了超冗余对强化学习算法带来的各方面的影响，然后在基于episode复位和密集奖励的前提下，提出了基于策略搜索类算法PPO和基于值函数类算法TD3的超冗余机械臂控制方法，在基于episode复位和稀疏奖励的前提下，提出基于HER的控制算法，实现了平面和空间工作空间内复位条件下任意目标的接近任务，在此基础上结合课程学习机制，采用逐步增大复位间隔的训练方式，实现了不复位条件下任意目标的接近任务。

最后，针对障碍下末端接近任务，本文首先验证了基于稀疏奖励的HER控制方式在不同障碍下的学习情况并进行了行为学分析，然后对于更加困难的障碍，提出了XXX算法，有效的解决了超冗余机械臂困难障碍下避障接近目标的任务学习。