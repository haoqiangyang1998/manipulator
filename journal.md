| 任务序号 | 任务内容 | 完成情况 |
| ---- | ---- | ---- |
|2021/3/9|  
|1|午休半小时|  done |
|2|两个小时准备专业实践答辩| done |
|3|构思运动跟随系统设计| 
|4|解析json文件| done | 
|5|尝试指数奖励和幂数奖励的训练效果| done |
|6|调研机器人高精度控制问题的文献|
|7|调研hindsight解释性文献和知乎百度解答| 
|8|解析12自由度关节生成策略的eval关节变化效果| 
|9|采用近似v型分布关节空间采样| done | 
|10|使用learning rate decay代码|
|11|思考为什么hindsight比密集奖励效果还要好|
|12|分析hindsight探索空间的变化|
|13|测试her策略在24关节下随机初始化随机目标的完成度| done |
|14|看完google-research kaggle分享|
|15|尝试reward clip和reward scale 并调研相关文献 | X |
|16|补充低次势函数的实验|
|17|添加末端 距离矢量特征| done |
|18|减去关节速度特征| done |
|19|测试her总体reward变化|
|20|0.15精度伪densenet,将状态同步| done |
|21|是否要像灵巧手一样区别actor和critic的输入状态| 
|22|是否要用四元数作为姿态比较好| 不需要 |
|23|测试重新规划状态|
|24|分布式训练环境搭建| X |
|25|测试密集奖励的训练效果，重新测试额外的奖励设置| done |
|26|测试action attention新idea bypass权重加在动作上|
|27| 测试6自由度永不复位 |
|28|测试固定间隔复位只用MLP的情况（这里）| done |
|29|测试状态添加爱每个关节的坐标，对比值添加爱到值网络和都添加，思考马尔可夫特性是否满足，累积误差是不是这个导致的，对特性做去哪买你对比分析，用td3密集奖励做比较好吧|
|30|将her改为td3版本验证24自由度效果| done |
|31|reward shaping全面实验，包括增加额外奖励|
|32|查看action l2的loss平衡不|
|33|思考为什么已经收敛的策略重新学不好学，查看overfiting文献|
|34|查看alphastar openaifive的网络深度|





